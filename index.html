
<!DOCTYPE html>
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-52138338-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-52138338-2');
    </script>


    <meta name="generator" content="HTML Tidy"></meta>
    <link href="stylesheets/style.css" rel="stylesheet" type="text/css"></link>
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic" rel="stylesheet" type="text/css"></link>
    <script src="js/hidebib.js" type="text/javascript"></script>
    <title>Sahil Singla</title>
    <description content="PhD Candidate in Computer Science"></description>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
    <td>
    <table width="100%" align="center" cellspacing="0" cellpadding="20">
      <p align="center">
        <font size="7">Sahil Singla</font>
        <br></br>
        <b>Email </b>
        <font id="email" style="display:inline;">ssingla (at) umd (dot) edu</font>
      </p>

      <td width="35%" valign="top">
        <a href="images/profile.jpg">
          <img src="images/profile.jpg" width="95%" />
        </a>
      </td>
      <td width="60%" valign="middle" align="justify">
        <div id="includedContent"></div>
        <p>
          <p>
              <strong>I am on the job market!</strong>
          </p>
          <p>
              I am a fourth year PhD student in Artificial Intelligence at the <a href="https://www.umd.edu/">University of Maryland, College Park</a>, where I am advised by <a href="https://www.cs.umd.edu/~sfeizi/"> Prof. Soheil Feizi </a>. Prior to joining UMD, I obtained a Bachelor's degree (B.Tech) with a major in Computer Science from <a href="https://www.iitd.ac.in/"> Indian Institute of Technology, Delhi </a>.
          </p>
          <p>
              I work on problems in robust and reliable machine learning, more specifically on adversarial robustness and discovering failure modes of deep neural networks.
          </p>

        </p>

        <p align="center"> 
          <a href="https://scholar.google.com/citations?user=jjjbOI4AAAAJ&hl=en">Google Scholar</a> | <a href="https://github.com/singlasahil14">Github</a> | <a href="https://twitter.com/sahilsingla47">Twitter</a> | <a href="https://www.linkedin.com/in/sahil-singla-94112141/">LinkedIn</a> | <a href="docs/Resume.pdf">Resume</a>
        </p>
      </td>

    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <sectionheading id="news">News</sectionheading>
          <ul>
              <li>I gave a talk at the London Machine Learning meetup.</li>
              <li>Two papers accepted at ICLR 2022.</li>
          </ul>
        </td>
      </tr>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <sectionheading>Publications</sectionheading>
        </td>
      </tr>
    </table>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

      <tr>
        <td>
          <h2> <font color="gray">2022 </font> </h2>
        </td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://openreview.net/pdf?id=tD7eCtaSkR">
          <a href="https://openreview.net/pdf?id=tD7eCtaSkR">
            <img src="images/teaser_figures/2022ICLRrobustness.jpg" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top">
          <a href="https://openreview.net/pdf?id=tD7eCtaSkR" id="2022ICLRrobustness_paper"><paper_title>Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100</paper_title></a>
          <br><strong>Sahil Singla</strong>, Surbhi Singla, Soheil Feizi.<br> <em>ICLR</em>, 2022 (spotlight)<br><div class="paper" id="2022ICLRrobustness">
            <a href="https://openreview.net/pdf?id=tD7eCtaSkR">pdf</a>
            | <a href="javascript:toggleblock('2022ICLRrobustness_abs')">abstract</a>
            | <a href="javascript:toggleblock('2022ICLRrobustness_bib')">bibtex</a>
            | <a href="https://github.com/singlasahil14/SOC">code</a><br>
            <p align="justify"><i id="2022ICLRrobustness_abs">
              Training convolutional neural networks (CNNs) with a strict Lipschitz constraint under the l2 norm is useful for provable adversarial robustness, interpretable gradients and stable training. While 1-Lipschitz CNNs can be designed by enforcing a 1-Lipschitz constraint on each layer, training such networks requires each layer to have an orthogonal Jacobian matrix (for all inputs) to prevent the gradients from vanishing during backpropagation. A layer with this property is said to be Gradient Norm Preserving (GNP). In this work, we introduce a procedure to certify the robustness of 1-Lipschitz CNNs by relaxing the orthogonalization of the last linear layer of the network that significantly advances the state of the art for both standard and provable robust accuracies on CIFAR-100 (gains of 4.80% and 4.71%, respectively). We further boost their robustness by introducing (i) a novel Gradient Norm preserving activation function called the Householder activation function (that includes every GroupSort activation) and (ii) a certificate regularization. On CIFAR-10, we achieve significant improvements over prior works in provable robust accuracy (5.81%) with only a minor drop in standard accuracy (âˆ’0.29%). 
            </i></p>
            <p align="justify"><i id="2022ICLRrobustness_bib">
              @inproceedings{<br>
                singla2022improved,<br>
                title={Improved deterministic l2 
                robustness on {CIFAR}-10 and 
                {CIFAR}-100},<br>
                author={Sahil Singla and 
                Surbhi Singla and Soheil Feizi},<br>
                booktitle={International Conference 
                on Learning Representations},<br>
                year={2022},<br>
                url={https://openreview.net/forum?id=tD7eCtaSkR}<br>
              }
            </i></p>
        </div></td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/2110.04301">
          <a href="https://arxiv.org/abs/2110.04301">
            <img src="images/teaser_figures/2022ICLRsalient.jpg" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top">
          <a href="https://arxiv.org/abs/2110.04301" id="2022ICLRsalient_paper"><paper_title>Salient ImageNet: How to discover spurious features in Deep Learning?</paper_title></a>
          <br><strong>Sahil Singla</strong>, Soheil Feizi. <br> <em>ICLR</em>, 2022<br><div class="paper" id="2022ICLRsalient">
            <a href="https://arxiv.org/abs/2110.04301">pdf</a>
            | <a href="javascript:toggleblock('2022ICLRsalient_abs')">abstract</a>
            | <a href="javascript:toggleblock('2022ICLRsalient_bib')">bibtex</a>
            | <a href="https://github.com/singlasahil14/salient_imagenet">code</a><br>
            <p align="justify"><i id="2022ICLRsalient_abs">
              A key reason for the lack of reliability of deep neural networks in the real world is their heavy reliance on spurious input features that are not essential to the true label. Focusing on image classifications, we define core attributes as the set of visual features that are always a part of the object definition while spurious attributes are the ones that are likely to co-occur with the object but not a part of it (e.g., attribute "fingers" for class "band aid"). Traditional methods for discovering spurious features either require extensive human annotations (thus, not scalable), or are useful on specific models. In this work, we introduce a general framework to discover a subset of spurious and core visual attributes used in inferences of a general model and localize them on a large number of images with minimal human supervision. Our methodology is based on this key idea: to identify spurious or core visual attributes used in model predictions, we identify spurious or core neural features (penultimate layer neurons of a robust model) via limited human supervision (e.g., using top 5 activating images per feature). We then show that these neural feature annotations generalize extremely well to many more images without any human supervision. We use the activation maps for these neural features as the soft masks to highlight spurious or core visual attributes. Using this methodology, we introduce the Salient Imagenet dataset containing core and spurious masks for a large set of samples from Imagenet. Using this dataset, we show that several popular Imagenet models rely heavily on various spurious features in their predictions, indicating the standard accuracy alone is not sufficient to fully assess model' performance specially in safety-critical applications.
            </i></p>
            <p align="justify"><i id="2022ICLRsalient_bib">
              @inproceedings{<br>
                singla2022salient,<br>
                title={Salient ImageNet: How to 
                discover spurious features in 
                Deep Learning?},<br>
                author={Sahil Singla and 
                Soheil Feizi},<br>
                booktitle={International 
                Conference on Learning 
                Representations},<br>
                year={2022},<br>
                url={https://openreview.net/forum?id=XVPqLyNxSyh}<br>
              }
            </i></p>
        </div></td>
      </tr>

      <tr>
        <td>
          <h2> <font color="gray">2021 </font> </h2>
        </td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/2105.11417">
          <a href="https://arxiv.org/abs/2105.11417">
            <img src="images/teaser_figures/2021ICML.jpg" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top">
          <a href="https://arxiv.org/abs/2105.11417" id="2021SOC_paper"><paper_title>Skew Orthogonal Convolutions</paper_title></a>
          <br><strong>Sahil Singla</strong>, Soheil Feizi.<br><em>ICML</em>, 2021<br><div class="paper" id="2021SOC">
            <a href="https://arxiv.org/abs/2105.11417">pdf</a>
            | <a href="javascript:toggleblock('2021SOC_abs')">abstract</a>
            | <a href="javascript:toggleblock('2021SOC_bib')">bibtex</a>
            | <a href="https://github.com/singlasahil14/SOC">code</a><br>
            <p align="justify"><i id="2021SOC_abs">
              Training convolutional neural networks with a Lipschitz constraint under the l2 norm is useful for provable adversarial robustness, interpretable gradients, stable training, etc. While 1-Lipschitz networks can be designed by imposing a 1-Lipschitz constraint on each layer, training such networks requires each layer to be gradient norm preserving (GNP) to prevent gradients from vanishing. However, existing GNP convolutions suffer from slow training, lead to significant reduction in accuracy and provide no guarantees on their approximations. In this work, we propose a GNP convolution layer called Skew Orthogonal Convolution (SOC) that uses the following mathematical property: when a matrix is {\it Skew-Symmetric}, its exponential function is an {\it orthogonal} matrix. To use this property, we first construct a convolution filter whose Jacobian is Skew-Symmetric. Then, we use the Taylor series expansion of the Jacobian exponential to construct the SOC layer that is orthogonal. To efficiently implement SOC, we keep a finite number of terms from the Taylor series and provide a provable guarantee on the approximation error. Our experiments on CIFAR-10 and CIFAR-100 show that SOC allows us to train provably Lipschitz, large convolutional neural networks significantly faster than prior works while achieving significant improvements for both standard and certified robust accuracies. 
            </i></p>
            <p align="justify"><i id="2021SOC_bib">
              @inproceedings{<br>
                singlafeiziSOC2021,<br>
                title={Skew Orthogonal Convolutions},<br>
                author={Singla, Sahil and Feizi, Soheil},<br>
                booktitle={Proceedings 
                of the 38th International 
                Conference on Machine 
                Learning},<br>
                year={2021},<br>
                url={https://proceedings.mlr.press/v139/singla21a.html}<br>
              }
            </i></p>
        </div></td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/2012.01750">
          <a href="https://arxiv.org/abs/2012.01750">
            <img src="images/teaser_figures/2021CVPR.jpg" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top">
          <a href="https://arxiv.org/abs/2012.01750" id="2021CVPR_paper"><paper_title>Understanding Failures of Deep Networks via Robust Feature Extraction</paper_title></a>
          <br><strong>Sahil Singla</strong>, Besmira Nushi, Shital Shah, Ece Kamar, Eric Horvitz.<br><em>CVPR</em>, 2021 (Oral) <br><div class="paper" id="2021CVPR">
            <a href="https://arxiv.org/abs/2012.01750">pdf</a>
            | <a href="javascript:toggleblock('2021CVPR_abs')">abstract</a>
            | <a href="javascript:toggleblock('2021CVPR_bib')">bibtex</a>
            | <a href="https://github.com/singlasahil14/barlow">code</a>
            | <a href="https://www.youtube.com/watch?v=m5xil1hFNfE">talk</a><br>
            <p align="justify"><i id="2021CVPR_abs">
              Traditional evaluation metrics for learned models that report aggregate scores over a test set are insufficient for surfacing important and informative patterns of failure over features and instances. We introduce and study a method aimed at characterizing and explaining failures by identifying visual attributes whose presence or absence results in poor performance. In distinction to previous work that relies upon crowdsourced labels for visual attributes, we leverage the representation of a separate robust model to extract interpretable features and then harness these features to identify failure modes. We further propose a visualization method aimed at enabling humans to understand the meaning encoded in such features and we test the comprehensibility of the features. An evaluation of the methods on the ImageNet dataset demonstrates that: (i) the proposed workflow is effective for discovering important failure modes, (ii) the visualization techniques help humans to understand the extracted features, and (iii) the extracted insights can assist engineers with error analysis and debugging.
            </i></p>
            <p align="justify"><i id="2021CVPR_bib">
              @inproceedings{<br>
                singlaCVPR2021,<br>
                title     = {Understanding Failures 
                of Deep Networks via Robust Feature 
                Extraction},<br>
                author    = {Sahil Singla and Besmira 
                Nushi and Shital Shah and Ece Kamar 
                and Eric Horvitz},<br>
                booktitle = {{IEEE} Conference on 
                Computer Vision and Pattern 
                Recognition, {CVPR} 2021},<br>
                publisher = {Computer Vision 
                Foundation / {IEEE}},<br>
                year = {2021},<br>
              }
            </i></p>
          </div></td>
      </tr>


      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/1911.10258">
          <a href="https://arxiv.org/abs/1911.10258">
            <img src="images/teaser_figures/2021ICLRfantastic.jpg" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top">
          <a href="https://arxiv.org/abs/1911.10258" id="2021ICLRfantastic_paper"><paper_title>Fantastic Four: Differentiable and Efficient Bounds on Singular Values of Convolution Layers</paper_title></a>
          <br><strong>Sahil Singla</strong>, Soheil Feizi.<br><em>ICLR</em>, 2021<br><div class="paper" id="2021ICLRfantastic">
            <a href="https://arxiv.org/abs/1911.10258">pdf</a>
            | <a href="javascript:toggleblock('2021ICLRfantastic_abs')">abstract</a>
            | <a href="javascript:toggleblock('2021ICLRfantastic_bib')">bibtex</a>
            | <a href="https://github.com/singlasahil14/fantastic-four">code</a> <br>
            <p align="justify"><i id="2021ICLRfantastic_abs">
              In deep neural networks, the spectral norm of the Jacobian of a layer bounds the factor by which the norm of a signal changes during forward/backward propagation. Spectral norm regularizations have been shown to improve generalization, robustness and optimization of deep learning methods. Existing methods to compute the spectral norm of convolution layers either rely on heuristics that are efficient in computation but lack guarantees or are theoretically-sound but computationally expensive. In this work, we obtain the best of both worlds by deriving {\it four} provable upper bounds on the spectral norm of a standard 2D multi-channel convolution layer. These bounds are differentiable and can be computed efficiently during training with negligible overhead. One of these bounds is in fact the popular heuristic method of Miyato et al. (multiplied by a constant factor depending on filter sizes). Each of these four bounds can achieve the tightest gap depending on convolution filters. Thus, we propose to use the minimum of these four bounds as a tight, differentiable and efficient upper bound on the spectral norm of convolution layers. We show that our spectral bound is an effective regularizer and can be used to bound either the lipschitz constant or curvature values (eigenvalues of the Hessian) of neural networks. Through experiments on MNIST and CIFAR-10, we demonstrate the effectiveness of our spectral bound in improving generalization and provable robustness of deep networks.
            </i></p>
            <p align="justify"><i id="2021ICLRfantastic_bib">
              @inproceedings{<br>
                singla2021fantastic,<br>
                title={Fantastic Four: Differentiable 
                and Efficient Bounds on Singular
                Values of Convolution Layers},<br>
                author={Sahil Singla and Soheil 
                Feizi},<br>
                booktitle={International Conference 
                on Learning Representations},<br>
                year={2021},<br>
                url={https://openreview.net/forum?id=JCRblSgs34Z}<br>
              }
            </i></p>
        </div></td>
      </tr>


      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/2006.12655">
          <a href="https://arxiv.org/abs/2006.12655">
            <img src="images/teaser_figures/2021ICLRperceptual.jpg" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://arxiv.org/abs/2006.12655" id="2021ICLRperceptual_paper">
          <paper_title>Perceptual Adversarial Robustness: Defense Against Unseen Threat Models</paper_title></a>
          <br>Cassidy Laidlaw, <strong>Sahil Singla</strong>, Soheil Feizi.<br><em>ICLR</em>, 2021<br>
          <div class="paper" id="2021ICLRperceptual"><a href="https://arxiv.org/abs/2006.12655">pdf</a>
            | <a href="javascript:toggleblock('2021ICLRperceptual_abs')">abstract</a>
            | <a href="javascript:toggleblock('2021ICLRperceptual_bib')">bibtex</a>
            | <a href="https://github.com/cassidylaidlaw/perceptual-advex">code</a> <br>
              <br>
              <p align="justify"><i id="2021ICLRperceptual_abs">
                A key challenge in adversarial robustness is the lack of a precise mathematical characterization of human perception, used in the very definition of adversarial attacks that are imperceptible to human eyes. Most current attacks and defenses try to avoid this issue by considering restrictive adversarial threat models such as those bounded by L2 or Lâˆž distance, spatial perturbations, etc. However, models that are robust against any of these restrictive threat models are still fragile against other threat models. To resolve this issue, we propose adversarial training against the set of all imperceptible adversarial examples, approximated using deep neural networks. We call this threat model the neural perceptual threat model (NPTM); it includes adversarial examples with a bounded neural perceptual distance (a neural network-based approximation of the true perceptual distance) to natural images. Through an extensive perceptual study, we show that the neural perceptual distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model.<br>
                Under the NPTM, we develop novel perceptual adversarial attacks and defenses. Because the NPTM is very broad, we find that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five diverse adversarial attacks. We find that PAT achieves state-of-the-art robustness against the union of these five attacks, more than doubling the accuracy over the next best model, without training against any of them. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the first adversarial training defense with this property.
              </i></p>
              <p align="justify"><i id="2021ICLRperceptual_bib">
                @inproceedings{<br>
                  laidlaw2021perceptual,<br>
                  title={Perceptual Adversarial
                  Robustness: Defense Against Unseen 
                  Threat Models},<br>
                  author={Cassidy Laidlaw and Sahil 
                  Singla and Soheil Feizi},<br>
                  booktitle={International Conference 
                  on Learning Representations},<br>
                  year={2021},<br>
                  url={https://openreview.net/forum?id=dFwBosAcJkN}<br>
                }
            </i></p>
          </div></td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/2102.07861">
          <a href="https://arxiv.org/abs/2102.07861">
            <img src="images/teaser_figures/2021ICCV.jpg" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://arxiv.org/abs/2102.07861" id="2021ICCV_paper">
          <paper_title>Low Curvature Activations Reduce Overfitting in Adversarial Training</paper_title></a>
          <br>Vasu Singla, <strong>Sahil Singla</strong>, Soheil Feizi, David Jacobs.<br><em>ICCV</em>, 2021<br>
          <div class="paper" id="2020arXivInstanceAdaptive"><a href="https://arxiv.org/abs/2102.07861">pdf</a>
            | <a href="javascript:toggleblock('2021ICCV_abs')">abstract</a>
            | <a href="javascript:toggleblock('2021ICCV_bib')">bibtex</a>
            | <a href="https://github.com/vasusingla/low_curvature_activations">code</a> <br>
            <p align="justify"><i id="2021ICCV_abs">
                Adversarial training is one of the most effective defenses against adversarial attacks. Previous works suggest that overfitting is a dominant phenomenon in adversarial training leading to a large generalization gap between test and train accuracy in neural networks. In this work, we show that the observed generalization gap is closely related to the choice of the activation function. In particular, we show that using activation functions with low (exact or approximate) curvature values has a regularization effect that significantly reduces both the standard and robust generalization gaps in adversarial training. We observe this effect for both differentiable/smooth activations such as SiLU as well as non-differentiable/non-smooth activations such as LeakyReLU. In the latter case, the "approximate" curvature of the activation is low. Finally, we show that for activation functions with low curvature, the double descent phenomenon for adversarially trained models does not occur.
              </i></p>
              <p align="justify"><i id="2021ICCV_bib">
                @inproceedings{<br>
                    Singla_2021_ICCV,<br>
                    author    = {Singla, Vasu and Singla, Sahil and Feizi, Soheil and Jacobs, David},<br>
                    title     = {Low Curvature Activations Reduce Overfitting in Adversarial Training},<br>
                    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},<br>
                    month     = {October},<br>
                    year      = {2021},<br>
                    pages     = {16423-16433}<br>
                }
              </i></p>
          </div></td>
      </tr>



      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/2006.12621">
          <a href="https://arxiv.org/abs/2006.12621">
            <img src="images/teaser_figures/2021FAccT.jpg" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://arxiv.org/abs/2006.12621" id="2021FAccT_paper">
          <paper_title>Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning</paper_title></a>
          <br>Vedant Nanda, Samuel Dooley, <strong>Sahil Singla</strong>, Soheil Feizi, John P. Dickerson.<br><em>FAccT</em>, 2021<br>
          <div class="paper" id="2021FAccT"><a href="https://arxiv.org/abs/2006.12621">pdf</a>
            | <a href="javascript:toggleblock('2021FAccT_abs')">abstract</a>
            | <a href="javascript:toggleblock('2021FAccT_bib')">bibtex</a>
            | <a href="https://github.com/nvedant07/Fairness-Through-Robustness">code</a> <br>
            <p align="justify"><i id="2021FAccT_abs">
                Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of \textit{robustness bias}. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: <a href="https://github.com/nvedant07/Fairness-Through-Robustness">this https URL</a>.
              </i></p>
              <p align="justify"><i id="2021FAccT_bib">
                @inproceedings{<br>
                  nanda2021fairness,<br>
                  author = {Nanda, Vedant and Dooley, Samuel and Singla, Sahil and Feizi, Soheil and Dickerson, John P.},<br>
                  title = {Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning},<br>
                  year = {2021},<br>
                  isbn = {9781450383097},<br>
                  publisher = {Association for Computing Machinery},<br>
                  address = {New York, NY, USA},<br>
                  url = {https://doi.org/10.1145/3442188.3445910},<br>
                  doi = {10.1145/3442188.3445910},<br>
                  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},<br>
                  pages = {466â€“477},<br>
                  numpages = {12},<br>
                  location = {Virtual Event, Canada},<br>
                  series = {FAccT '21}<br>
                }
            </i></p>
          </div></td>
      </tr>



      <tr>
        <td>
          <h2> <font color="gray">2020 </font> </h2>
        </td>
      </tr>


      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/2006.00731">
          <a href="https://arxiv.org/abs/2006.00731">
            <img src="images/teaser_figures/2020ICML.jpg" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://arxiv.org/abs/2006.00731" id="2020ICML_paper">
          <paper_title>Second-Order Provable Defenses against Adversarial Attacks</paper_title></a>
          <br><strong>Sahil Singla</strong>, Soheil Feizi.<br><em>ICML</em>, 2020<br>
          <div class="paper" id="2020ICML"><a href="https://arxiv.org/abs/2006.00731">pdf</a>
            | <a href="javascript:toggleblock('2020ICML_abs')">abstract</a>
            | <a href="javascript:toggleblock('2020ICML_bib')">bibtex</a>
            | <a href="https://github.com/singlasahil14/so-robust">code</a><br>
            <p align="justify"><i id="2020ICML_abs">
                Understanding proper distance measures between distributions is at the core of several learning tasks
                  such as generative models, domain adaptation, clustering, etc. In this work, we focus on mixture
                  distributions that arise naturally in several application domains where the data contains different
                  sub-populations. For mixture distributions, established distance measures such as the Wasserstein
                  distance do not take into account imbalanced mixture proportions. Thus, even if two mixture
                  distributions have identical mixture components but different mixture proportions, the Wasserstein
                  distance between them will be large. This often leads to undesired results in distance-based learning
                  methods for mixture distributions. In this paper, we resolve this issue by introducing the Normalized
                  Wasserstein measure. The key idea is to introduce mixture proportions as optimization variables,
                  effectively normalizing mixture proportions in the Wasserstein formulation. Using the proposed normalized
                  Wasserstein measure leads to significant performance gains for mixture distributions with imbalanced
                  mixture proportions compared to the vanilla Wasserstein distance. We demonstrate the effectiveness
                  of the proposed measure in GANs, domain adaptation and adversarial clustering in several benchmark
                  datasets.
              </i></p>
              <p align="justify"><i id="2020ICML_bib">
                @inproceedings{<br>
                  singlaCRT2020ICML,<br>
                  title =    {Second-Order Provable Defenses against Adversarial Attacks},<br>
                  author =       {Singla, Sahil and Feizi, Soheil},<br>
                  booktitle =    {Proceedings of the 37th International Conference on Machine Learning},<br>
                  year =   {2020},<br>
                  url =    {https://proceedings.mlr.press/v119/singla20a.html}<br>
                }
              </i></p>
          </div></td>
      </tr>

      <tr>
        <td>
          <h2> <font color="gray">2019 </font> </h2>
        </td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/1902.00407">
          <a href="https://arxiv.org/abs/1902.00407">
            <img src="images/teaser_figures/2019ICML.jpg" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://arxiv.org/abs/1902.00407" id="2019ICML_paper">
          <paper_title>Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation</paper_title></a>
          <br><strong>Sahil Singla</strong>, Eric Wallace, Shi Feng, Soheil Feizi.<br><em>ICML</em>, 2019<br>
          <div class="paper" id="2019ICML"><a href="https://arxiv.org/abs/1902.00407">pdf</a>
            | <a href="javascript:toggleblock('2019ICML_abs')">abstract</a>
            | <a href="javascript:toggleblock('2019ICML_bib')">bibtex</a>
            | <a href="https://github.com/singlasahil14/CASO">code</a>
            <br>
            <p align="justify"><i id="2019ICML_abs">
                Current saliency map interpretations for neural networks generally rely on two key assumptions. First, they use first-order approximations of the loss function, neglecting higher-order terms such as the loss curvature. Second, they evaluate each featureâ€™s importance in isolation, ignoring feature interdependencies. This work studies the effect of relaxing these two assumptions. First, we characterize a closed-form formula for the input Hessian matrix of a deep ReLU network. Using this formula, we show that, for classification problems with many classes, if a prediction has high probability then including the Hessian term has a small impact on the interpretation. We prove this result by demonstrating that these conditions cause the Hessian matrix to be approximately rank one and its leading eigenvector to be almost parallel to the gradient of the loss. We empirically validate this theory by interpreting ImageNet classifiers. Second, we incorporate feature interdependencies by calculating the importance of group-features using a sparsity regularization term. We use an L0 - L1 relaxation technique along with proximal gradient descent to efficiently compute group-feature importance values. Our empirical results show that our method significantly improves deep learning interpretations.
            </i></p>
            <p align="justify"><i id="2019ICML_bib">
              @inproceedings{<br>
                singlaCASO2019,<br>
                title =    {Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation},<br>
                author =       {Singla, Sahil and Wallace, Eric and Feng, Shi and Feizi, Soheil},<br>
                booktitle =    {Proceedings of the 36th International Conference on Machine Learning},<br>
                year =   {2019},<br>
                url =    {https://proceedings.mlr.press/v97/singla19a.html}<br>
              }
            </i></p>
          </div></td>
      </tr>



    <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td>
          <sectionheading>Internships</sectionheading>
          <ul>
            <li> <a href="https://www.microsoft.com/en-us/research/">Microsoft Research </a> (Summer 2020) <br>Topic: Failure expanation of deep neural networks. </li>
          </ul>
        </td>
      </tr>
    </table>

    <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td>
          <sectionheading>Selected Awards</sectionheading>
          <ul>
            <li> <strong>Outstanding Research Assistant Award.</strong> Awarded to top 2% graduate research assistants every year by the Graduate School at the University of Maryland.</li>
            <li> <strong>Dean's Fellowship.</strong> Awarded to only two students in the first and second year in the Computer Science department at University of Maryland.</li>
          </ul>
        </td>
      </tr>
    </table>
  </body>
  <script xml:space="preserve" language="JavaScript">hideallbibs();</script>
  <script xml:space="preserve" language="javascript">hideblock('2022ICLRrobustness_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2022ICLRrobustness_bib');</script>
  <script xml:space="preserve" language="javascript">hideblock('2022ICLRsalient_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2022ICLRsalient_bib');</script>
  <script xml:space="preserve" language="javascript">hideblock('2021SOC_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2021SOC_bib');</script>
  <script xml:space="preserve" language="javascript">hideblock('2021CVPR_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2021CVPR_bib');</script>
  <script xml:space="preserve" language="javascript">hideblock('2021ICLRfantastic_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2021ICLRfantastic_bib');</script>
  <script xml:space="preserve" language="javascript">hideblock('2021ICLRperceptual_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2021ICLRperceptual_bib');</script>
  <script xml:space="preserve" language="javascript">hideblock('2021ICCV_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2021ICCV_bib');</script>
  <script xml:space="preserve" language="javascript">hideblock('2021FAccT_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2021FAccT_bib');</script>
  <script xml:space="preserve" language="javascript">hideblock('2020ICML_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2020ICML_bib');</script>
  <script xml:space="preserve" language="javascript">hideblock('2019ICML_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2019ICML_bib');</script>


</html>
